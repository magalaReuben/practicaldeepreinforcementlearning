{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8ad8111-4762-47ca-be1a-2980f724a3f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Policy Gradient Based Reinforcement Learning\n",
    "The goal of any Reinforcement Learning(RL) algorithm is to determine the optimal policy that has a maximum reward. Policy gradient methods are policy iterative method that means modelling and optimising the policy directly.\n",
    "\n",
    "**Key Idea:**\n",
    "\n",
    "-   **Directly Learn the Optimal Policy:** Instead of estimating value functions (like Q-values) as a stepping stone, policy-based methods focus on directly finding the optimal policy (œÄ*) that maximizes expected cumulative reward.\n",
    "-   **Parameterize the Policy:** The policy is represented as a function with adjustable parameters (Œ∏), often a neural network, that maps states to actions.\n",
    "-   **Output a Probability Distribution:** This parameterized policy (œÄ_Œ∏) outputs a probability distribution over possible actions for a given state, indicating the likelihood of taking each action. This makes it a stochastic policy, allowing for exploration.\n",
    "\n",
    "![enter image description here](https://live.staticflickr.com/65535/53425957386_4aa44779a2_c.jpg)\n",
    "**Goal:**\n",
    "\n",
    "-   In policy-based methods, we aim to directly learn the optimal policy œÄ* that maximizes the expected cumulative reward over time.\n",
    "-   To achieve this, we need to find the best set of parameters Œ∏ for our parameterized policy œÄ_Œ∏.\n",
    "\n",
    "**Gradient Ascent for Policy Improvement:**\n",
    "\n",
    "2.  **Performance Measure:** We define a performance measure (objective function), often the expected cumulative reward, to evaluate the quality of the policy.\n",
    "4.  **Policy Gradient:** We calculate the gradient of this performance measure with respect to the policy parameters Œ∏. This gradient indicates how small changes in Œ∏ would affect the expected reward.\n",
    "6.  **Parameter Updates:** We update Œ∏ in the direction of the gradient using gradient ascent. This means adjusting Œ∏ to make actions that led to higher rewards more likely in the future.\n",
    "\n",
    "**Equation:**\n",
    "\n",
    "-   The update rule for gradient ascent is typically:\n",
    "    \n",
    "    Œ∏ ‚Üê Œ∏ + Œ± ‚àáŒ∏ J(Œ∏)\n",
    "    \n",
    "    where:\n",
    "    \n",
    "    -   Œ∏: the policy parameters\n",
    "    -   Œ±: the learning rate (controlling the step size)\n",
    "    -   ‚àáŒ∏ J(Œ∏): the gradient of the performance measure J with respect to Œ∏\n",
    "    \n",
    "\n",
    "**Controlling Action Distributions:**\n",
    "\n",
    "-   By adjusting Œ∏ through gradient ascent, we indirectly shape the probability distribution over actions that the policy outputs for each state.\n",
    "-   This means making actions that have historically led to better outcomes more probable, while those associated with lower rewards become less likely.\n",
    "-   Over time, this process guides the policy towards behaviors that maximize the expected cumulative reward.\n",
    "\n",
    "**Advantages of Policy-Based Methods in Reinforcement Learning:**\n",
    "\n",
    "**1. Direct Policy Learning:**\n",
    "\n",
    "-   **Focus on the Goal:** Directly optimize the policy for maximizing expected reward, aligning with the ultimate aim of RL.\n",
    "-   **No Intermediate Value Function:** Eliminate the need to learn and store value functions (like Q-values), potentially reducing computational overhead and simplifying model design.\n",
    "\n",
    "**2. Handling Stochastic Policies:**\n",
    "-   **Exploration Built-In:** Naturally explore different actions due to probabilistic action selection, reducing the need for manual exploration strategies.\n",
    "\n",
    "\n",
    " **3. Addressing Perceptual Aliasing:** \n",
    "\n",
    "Effectively handle scenarios where distinct actions are required in seemingly identical states, preventing suboptimal behavior.\n",
    "\n",
    "**4. Continuous Action Spaces:**\n",
    "\n",
    "-   **No Discretization Needed:** Learn policies for continuous action spaces without discretization, enabling smooth and adaptable control in environments with fine-grained actions.\n",
    "\n",
    "**5. Learning Complex Behaviors:**\n",
    "\n",
    "-   **Model Uncertainty:** Capture uncertainty and risk preferences through stochastic policies, potentially leading to more robust and adaptive behaviors.\n",
    "-   **Explore Diverse Strategies:** Discover creative or unorthodox solutions that might be overlooked by deterministic approaches.\n",
    "\n",
    "**Additional Advantages:**\n",
    "\n",
    "-   **Gradient-Based Optimization:** Leverage powerful gradient-based techniques for efficient policy improvement.\n",
    "-   **Potential for Better Generalization:** May generalize better to unseen states or tasks in some cases.\n",
    "\n",
    "**Important Considerations:**\n",
    "\n",
    "-   **Hyperparameter Sensitivity:** Often require more careful tuning of hyperparameters compared to value-based methods.\n",
    "-   **Local Optima:** Not guaranteed to find the globally optimal policy, potentially converging to local optima in complex environments.\n",
    "-   **Variance Reduction Techniques:** May necessitate variance reduction techniques for stable learning, especially with high-dimensional action spaces.\n",
    "\n",
    "**When to Consider Policy-Based Methods:**\n",
    "\n",
    "-   **Continuous Action Spaces:** Essential for continuous action domains where value-based methods are impractical.\n",
    "-   **Stochasticity Benefits:** When exploration, uncertainty modeling, or tackling perceptual aliasing are crucial for optimal performance.\n",
    "-   **Complex or Stochastic Environments:** Can be advantageous in highly complex or stochastic environments where value functions might be difficult to learn accurately.\n",
    "\n",
    "**DIVING DEEPER** üòÑüòÑ (Images and some references from REINFORCE - a policy-gradient based reinforcement Learning algorithm by Dhanoop Karunakaran)\n",
    "\n",
    "Our objective with the policy-gradient approach is to influence the probability distribution of actions by adjusting the policy, ensuring that actions with higher returns are more likely to be chosen in future interactions. After each interaction with the environment, we fine-tune the parameters to favor the sampling of actions that lead to better outcomes.\n",
    "\n",
    "Now, the question arises: **How do we optimize the weights using the expected return?**\n",
    "\n",
    "We can define our return as the sum of rewards from the current state to the goal state, which essentially is the cumulative reward along the trajectory. In other words, it is the sum of rewards in a trajectory, considering a finite undiscounted horizon.\n",
    "\n",
    "![enter image description here](https://miro.medium.com/v2/resize:fit:640/format:webp/1*XGZsBdhxJ7n95f5HlOJdCA.png)\n",
    "\n",
    "The concept involves allowing the agent to engage in an episode. If the episode results in a victory, we interpret each action taken as beneficial and deserving of increased future sampling since they contributed to the success.\n",
    "\n",
    "For each state-action pair, the aim is to enhance the probability P(a|s) of taking that specific action in that state. This probability should be increased if the episode was won, indicating the action's effectiveness, or decreased if the episode was lost.\n",
    "![enter image description here](https://live.staticflickr.com/65535/53437294768_9e88fcc0b0_c.jpg)\n",
    "\n",
    "**Objective Function**\n",
    "  \n",
    "In policy gradient methods, the policy is commonly represented by a parameterized function denoted as œÄŒ∏(a|s), where Œ∏ is the set of parameters. Mathematically, an objective function seeks to either minimize or maximize a certain quantity. In the context of policy gradient, we work with a stochastic, parameterized policy, œÄŒ∏, and our goal is to maximize the expected return through the optimization of the objective function J(œÄŒ∏)\n",
    "![enter image description here](https://miro.medium.com/v2/resize:fit:828/format:webp/1*O2S1_x6jnwmwF9UlY3xXZA.png)\n",
    "The variable R(st, at) signifies the reward acquired at timestep t by executing action at from the state st. It is worth noting that this can be equivalently expressed as R(œÑ), where œÑ denotes a trajectory.\n",
    "\n",
    "To enhance the objective function J and consequently maximize the return, we adjust the policy parameter Œ∏ to derive the optimal policy. This optimal policy inherently seeks to maximize the overall return. The optimization process involves utilizing gradient ascent, an iterative algorithm, to systematically explore and refine the parameter values, ultimately converging towards the optimal set that maximizes the objective function.\n",
    "  \n",
    "If we are able to compute the gradient (‚àá) of the objective function J, as illustrated below:\n",
    "![enter image description here](https://miro.medium.com/v2/resize:fit:640/format:webp/1*UTuOFOpFSCHP42ukr-DoDw.png)\n",
    "Subsequently, we can modify the policy parameter Œ∏ (for simplicity, using Œ∏ instead of œÄŒ∏) by employing the gradient ascent rule. This entails updating the parameters Œ∏ in the direction of the gradient. It's crucial to bear in mind that the gradient not only provides the direction of the maximum change but also indicates the maximum rate of change.\n",
    "\n",
    "The gradient update rule is articulated as follows:\n",
    "![enter image description here](https://miro.medium.com/v2/resize:fit:640/format:webp/1*9VVtpB30epVxd23I-tuiEg.png)\n",
    "\n",
    "**Deeper deep  üòÑüòÑüêüüèä‚Äç‚ôÄÔ∏è**\n",
    "The expectation (E) of X is calculated by summing the product of each possible value (x) of X with its corresponding probability (P(x)), across all possible values of X.\n",
    "\n",
    "![enter image description here](https://miro.medium.com/v2/resize:fit:640/format:webp/1*QTj-keOqR5GygXu-PtdGNA.png)\n",
    "\n",
    "Hence our gradient can be re written as follows:\n",
    "![enter image description here](https://miro.medium.com/v2/resize:fit:828/format:webp/1*_HKQPf0i1oP7xptSHEhBog.png)\n",
    "Hence:\n",
    "![enter image description here](https://miro.medium.com/v2/resize:fit:828/format:webp/1*2Vru9C2sQPpLiTxQ5NQ3Iw.png)\n",
    "The probability of a trajectory with respect to the parameter Œ∏, denoted as P(œÑ|Œ∏), can be expanded as follows:\n",
    "![enter image description here](https://miro.medium.com/v2/resize:fit:828/format:webp/1*S-pYTGMKSQ1_JXrVlGb8Kw.png)\n",
    "Certainly, where p(s‚ÇÄ) represents the probability distribution of the initial state, and P(st+1|st, at) denotes the transition probability of transitioning to the new state st+1 by executing action at from the current state st.\n",
    "If we take the log-probability of the trajectory:\n",
    "![enter image description here](https://miro.medium.com/v2/resize:fit:828/format:webp/1*bW9doC9s18m1r2Yk4CPMVA.png)\n",
    "We can compute the gradient of the log-probability of a trajectory, and this computation yields:\n",
    "![enter image description here](https://miro.medium.com/v2/resize:fit:828/format:webp/1*_IEhX89w8Ph_sAMN5EE8Fg.png)\n",
    "We can modify this function as shown below based on the transition probability model, P(_st_+1‚Äã‚à£_st_‚Äã, _at_‚Äã) disappears because we are considering the model-free policy gradient algorithm where the transition probability model is not necessary\n",
    "![enter image description here](https://miro.medium.com/v2/resize:fit:828/format:webp/1*UP09SsUV0L_ZVCNpDkOdbA.png)\n",
    "Now the policy gradient expression is derived as\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:875/1*2Np3aqy7s5hpTpGIYfL4YQ.png)\n",
    "\n",
    "Policy gradient expression:\n",
    "\n",
    "The left-hand side of the equation can be replaced as below:\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:840/1*QCauEe13Qhhdr43n1ZLfdw.png)\n",
    "\n",
    "### REINFORCE (Monte Carlo Reinforce)\n",
    "REINFORCE is the Mote-Carlo sampling of policy gradient methods. That means the RL agent sample from starting state to goal state directly from the environment, rather than bootstrapping compared to other methods such as Temporal Difference Learning and Dynamic programming.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:875/1*GChX-STlUX9MGLq43wgwVA.png)\n",
    "\n",
    "Difference between Monte-Carlo, Temporal-Difference Learning, and Dynamic programming: Source: [10]\n",
    "\n",
    "We can rewrite our policy gradient expression in the context of Monte-Carlo sampling.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:868/1*LJxnqesbq-21b-Pv-_VkTA.png)\n",
    "\n",
    "REINFORCE expression: Source:[6] and [7]\n",
    "\n",
    "Where N is the number of trajectories is for one gradient update[6].\n",
    "\n",
    "### Pseudocode for the REINFORCE algorithm:\n",
    "\n",
    "1.  Sample N trajectories by following the policy œÄŒ∏.\n",
    "\n",
    "2. Evaluate the gradient using the below expression:\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:868/1*LJxnqesbq-21b-Pv-_VkTA.png)\n",
    "\n",
    "3. Update the policy parameters\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:363/1*iCMJVcpVg4y88zatLPDT9w.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c51b56a-4756-4d6a-b5f4-a73f19f1c8c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
