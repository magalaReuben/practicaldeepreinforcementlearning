{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/magalaReuben/practicaldeepreinforcementlearning/blob/main/Lecture3/DeepQLearningPytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsCaWuA8y9ez",
        "outputId": "d3850f6c-132b-41d1-fb0b-3c858c09262f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.2.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.2.0\n",
            "Collecting gymnasium[box2d]\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.9.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[box2d])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.2.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2376173 sha256=94ff7bfbbe05a4d129fd50f7dbdfd45d1d49d577a9cd8bddccf05aa2aa74bde7\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: farama-notifications, box2d-py, gymnasium\n",
            "Successfully installed box2d-py-2.3.5 farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ],
      "source": [
        "!pip install swig\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZljeOjoayCcc"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('LunarLander-v2')\n",
        "\n",
        "print(\"observation_space\", *env.observation_space.shape)\n",
        "print(\"action_space\", env.action_space.n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE-KMhD90ppc",
        "outputId": "9b5ad9a7-938a-4a3e-831c-dba471ccb3ef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation_space 8\n",
            "action_space 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "lQuVsDHa1pY5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "MEM_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self):\n",
        "        self.mem_count = 0\n",
        "\n",
        "        self.states = np.zeros((MEM_SIZE, *env.observation_space.shape),dtype=np.float32)\n",
        "        self.actions = np.zeros(MEM_SIZE, dtype=np.int64)\n",
        "        self.rewards = np.zeros(MEM_SIZE, dtype=np.float32)\n",
        "        self.states_ = np.zeros((MEM_SIZE, *env.observation_space.shape),dtype=np.float32)\n",
        "        self.dones = np.zeros(MEM_SIZE, dtype=bool)\n",
        "\n",
        "    def add(self, state, action, reward, state_, done):\n",
        "        mem_index = self.mem_count % MEM_SIZE\n",
        "\n",
        "        self.states[mem_index]  = state\n",
        "        self.actions[mem_index] = action\n",
        "        self.rewards[mem_index] = reward\n",
        "        self.states_[mem_index] = state_\n",
        "        self.dones[mem_index] =  1 - done\n",
        "\n",
        "        self.mem_count += 1\n",
        "\n",
        "    def sample(self):\n",
        "        MEM_MAX = min(self.mem_count, MEM_SIZE)\n",
        "        batch_indices = np.random.choice(MEM_MAX, BATCH_SIZE, replace=True)\n",
        "\n",
        "        states  = self.states[batch_indices]\n",
        "        actions = self.actions[batch_indices]\n",
        "        rewards = self.rewards[batch_indices]\n",
        "        states_ = self.states_[batch_indices]\n",
        "        dones   = self.dones[batch_indices]\n",
        "\n",
        "        return states, actions, rewards, states_, dones"
      ],
      "metadata": {
        "id": "Oje6pG5q2Rr4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 0.0001\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.input_shape = env.observation_space.shape\n",
        "        self.action_space = env.action_space.n\n",
        "\n",
        "        self.layer1 = nn.Linear(*self.input_shape, 1024)\n",
        "        self.layer2 = nn.Linear(512, 512)\n",
        "        self.layer3 = nn.Linear(512, self.action_space)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=LEARNING_RATE)\n",
        "        self.loss = nn.SmoothL1Loss()\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "_zcxNnul-ESt"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_epsilon = 1.0\n",
        "min_epsilon = 0.05\n",
        "decay_rate = 0.0005\n",
        "\n",
        "gamma = 0.95\n",
        "\n",
        "class DqnAgent:\n",
        "    def __init__(self):\n",
        "        self.memory = ReplayMemory()\n",
        "        self.epsilon = max_epsilon\n",
        "        self.network = DQN()\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        random_num = random.uniform(0, 1)\n",
        "        if random_num > self.epsilon:\n",
        "            state = torch.tensor(state).float().detach().to(device).unsqueeze(0)\n",
        "            q_values = self.network(state)\n",
        "            return torch.argmax(q_values).item()\n",
        "        else:\n",
        "            return env.action_space.sample()\n",
        "\n",
        "    def learn(self):\n",
        "        if self.memory.mem_count < BATCH_SIZE:\n",
        "            return\n",
        "\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample()\n",
        "        states = torch.tensor(states , dtype=torch.float32).to(device)\n",
        "        actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
        "        next_states = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
        "        dones = torch.tensor(dones, dtype=torch.bool).to(device)\n",
        "        batch_indices = np.arange(BATCH_SIZE, dtype=np.int64)\n",
        "\n",
        "        q_values = self.network(states)\n",
        "        next_q_values = self.network(next_states)\n",
        "\n",
        "        predicted_value_of_now = q_values[batch_indices, actions]\n",
        "        predicted_value_of_future = torch.max(next_q_values, dim=1)[0]\n",
        "\n",
        "        q_target = rewards + gamma * predicted_value_of_future * dones\n",
        "\n",
        "        loss = self.network.loss(q_target, predicted_value_of_now)\n",
        "        self.network.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.network.optimizer.step()\n",
        "\n",
        "        self.exploration_rate *= decay_rate\n",
        "        self.exploration_rate = max(min_epsilon, self.exploration_rate)\n",
        "\n",
        "    def returning_epsilon(self):\n",
        "        return self.exploration_rate"
      ],
      "metadata": {
        "id": "e7Ma_jBd_cFt"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPISODES = 1000\n",
        "best_reward = 0\n",
        "average_reward = 0\n",
        "episode_number = []\n",
        "average_reward_number = []\n",
        "observation_space = env.observation_space.shape[0]\n",
        "\n",
        "agent = DqnAgent()\n",
        "\n",
        "for i in range(1, EPISODES):\n",
        "    state = env.reset()[0]\n",
        "    state = np.reshape(state, [1, observation_space])\n",
        "    score = 0\n",
        "\n",
        "    while True:\n",
        "        #env.render()\n",
        "        action = agent.choose_action(state)\n",
        "        state_, reward, done, info = env.step(action)\n",
        "        state_ = np.reshape(state_, [1, observation_space])\n",
        "        agent.memory.add(state, action, reward, state_, done)\n",
        "        agent.learn()\n",
        "        state = state_\n",
        "        score += reward\n",
        "\n",
        "        if done:\n",
        "            if score > best_reward:\n",
        "                best_reward = score\n",
        "            average_reward += score\n",
        "            print(\"Episode {} Average Reward {} Best Reward {} Last Reward {} Epsilon {}\".format(i, average_reward/i, best_reward, score, agent.returning_epsilon()))\n",
        "            break\n",
        "\n",
        "        episode_number.append(i)\n",
        "        average_reward_number.append(average_reward/i)\n",
        "\n",
        "plt.plot(episode_number, average_reward_number)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ySZ2g0l5mQh8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPPCeYBcsgKAqbDVv8o5z0Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}