{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dead0ff-4de6-4ad4-9a51-575949a8e907",
   "metadata": {},
   "source": [
    "## Proximal Policy Optimization\n",
    "  \n",
    "Today, we're delving into Proximal Policy Optimization (PPO), a cutting-edge architecture designed to enhance the stability of our agent's training process by mitigating excessively large policy updates. PPO achieves this by employing a ratio that measures the variance between our current and previous policies, which is then carefully bounded within a defined range [1-ε, 1+ε]. By constraining this ratio, we effectively prevent policy updates from being overly drastic, thus fostering a more stable and reliable training environment.\n",
    "In essence, Proximal Policy Optimization (PPO) revolves around enhancing the stability of training by curbing the extent of policy adjustments made during each epoch. This precaution is crucial for two primary reasons:\n",
    "\n",
    "Firstly, empirical evidence suggests that **smaller policy updates throughout training tend to converge more reliably towards optimal solutions**.\n",
    "\n",
    "Secondly, **excessively large steps** in policy updates can lead to undesirable outcomes, akin to metaphorically \"**falling off the cliff**,\" resulting in suboptimal policies. Recovering from such setbacks may prove arduous, if not impossible, potentially prolonging the training process or hindering progress altogether.\n",
    "\n",
    "  \n",
    "In PPO, policy updates are approached with a conservative mindset. This entails quantifying the disparity between the current policy and its predecessor through a calculated ratio. By constraining this ratio within a defined range [1-ε, 1+ε] (**one minus epsilon, comma, one plus epsilon**), we effectively eliminate the temptation for the current policy to stray too far from its predecessor. This encapsulates the essence of the \"proximal policy\" notion, as we aim to keep policy adjustments within a proximate vicinity of the previous policy, thus fostering stability and mitigating drastic deviations.\n",
    "\n",
    "## The Clipped Surrogate Objective Function\n",
    "This was the objective to optmize in our reinforce algorithim:\n",
    "![enter image description here](https://live.staticflickr.com/65535/53540221027_1e500d0a76_z.jpg)\n",
    "  \n",
    "The idea was to help our agent choose actions that get better rewards and avoid bad ones. We tried to do this by adjusting our strategy using a math formula. But we ran into a problem with how big of a step we should take:\n",
    "\n",
    "-   If we took small steps, it took forever to train.\n",
    "-   If we took big steps, things got too unpredictable due to high variance.\n",
    "\n",
    "With PPO, we came up with a new plan. Instead of changing our strategy too much at once, we use a method called the Clipped Surrogate Objective Function. This method keeps our strategy changes in check, preventing them from getting too extreme. It helps us avoid making big mistakes during training.\n",
    "\n",
    "This new function **is designed to avoid destructively large weights updates** :\n",
    "![enter image description here](https://live.staticflickr.com/65535/53541114956_ae2e267c31_b.jpg)\n",
    "\n",
    "## Let's dive deeper and understnad this equation:\n",
    "### The Ratio Function\n",
    "![enter image description here](https://live.staticflickr.com/65535/53541314648_ce9f327921_b.jpg)\n",
    "\n",
    "The ratio is represented by this equation:\n",
    "![enter image description here](https://live.staticflickr.com/65535/53541564455_0a91b38977_b.jpg)\n",
    "\n",
    "This is about the likelihood of choosing an action 'a' at state 's' in the current policy compared to the previous one.\n",
    "\n",
    "The probability ratio, denoted by 'r(θ)', tells us:\n",
    "\n",
    "-   If **r(θ) > 1**, it means the action 'a' at state 's' is more probable in the current policy than the old one.\n",
    "-   If r(θ) is between 0 and 1, it means the action is less likely in the current policy than before.\n",
    "\n",
    "So, this probability ratio helps us understand how much the current policy differs from the old one.\n",
    "\n",
    "## The unclipped part of the Clipped Surrogate Objective function\n",
    "![enter image description here](https://live.staticflickr.com/65535/53541518364_41dac7a018_b.jpg)\n",
    "This ratio serves as a substitute for the log probability typically utilized in the policy objective function. It effectively forms the initial component of the new objective function by simply multiplying the ratio by the advantage.\n",
    "![enter image description here](https://live.staticflickr.com/65535/53541202901_e0d4016a06_b.jpg)\n",
    "\n",
    "## The clipped Part of the Clipped Surrogate Objective function\n",
    "![enter image description here](https://live.staticflickr.com/65535/53541651550_8de3844519_b.jpg)\n",
    "over time, it's crucial to limit the fluctuations of our objective function by penalizing deviations that stray too far from a ratio of 1. In the literature, this ratio is typically restricted to a range of 0.8 to 1.2.\n",
    "\n",
    "Clipping the ratio ensures that policy updates remain within reasonable bounds, preventing drastic deviations from the previous policy. To achieve this, we have two main approaches:\n",
    "\n",
    "1.  **Trust Region Policy Optimization (TRPO):** This method employs KL divergence constraints external to the objective function to regulate the policy update. However, TRPO tends to be complex to implement and demands more computational resources.\n",
    "    \n",
    "2.  **Proximal Policy Optimization (PPO):** PPO directly incorporates clipping of the probability ratio within the objective function through its Clipped Surrogate Objective. This streamlined approach simplifies implementation and computational requirements.\n",
    "\n",
    "![enter image description here](https://live.staticflickr.com/65535/53541597074_6d93327333_b.jpg)\n",
    "\n",
    "  \n",
    "In this modified version, the clipped component constrains the probability ratio, denoted by ��(�)rt​(θ), within a range of [1−�,1+�][1−ϵ,1+ϵ].\n",
    "\n",
    "With the Clipped Surrogate Objective function, we work with two versions of the probability ratio: one unclipped and another clipped within the specified range. Here, epsilon serves as a hyperparameter defining the clip range (typically set to 0.2 in the literature).\n",
    "\n",
    "Subsequently, we compute the objective by selecting the minimum value between the clipped and unclipped versions. This approach establishes a lower bound (a pessimistic bound) for the unclipped objective.\n",
    "\n",
    "Opting for the minimum between the clipped and unclipped objectives ensures that we choose the most conservative estimate based on the ratio and advantage circumstance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae00f68-bb1d-4672-b758-18ac37f8e5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
