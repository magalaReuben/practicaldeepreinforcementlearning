{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dead0ff-4de6-4ad4-9a51-575949a8e907",
   "metadata": {},
   "source": [
    "## Proximal Policy Optimization\n",
    "  \n",
    "Today, we're delving into Proximal Policy Optimization (PPO), a cutting-edge architecture designed to enhance the stability of our agent's training process by mitigating excessively large policy updates. PPO achieves this by employing a ratio that measures the variance between our current and previous policies, which is then carefully bounded within a defined range [1-ε, 1+ε]. By constraining this ratio, we effectively prevent policy updates from being overly drastic, thus fostering a more stable and reliable training environment.\n",
    "In essence, Proximal Policy Optimization (PPO) revolves around enhancing the stability of training by curbing the extent of policy adjustments made during each epoch. This precaution is crucial for two primary reasons:\n",
    "\n",
    "Firstly, empirical evidence suggests that **smaller policy updates throughout training tend to converge more reliably towards optimal solutions**.\n",
    "\n",
    "Secondly, **excessively large steps** in policy updates can lead to undesirable outcomes, akin to metaphorically \"**falling off the cliff**,\" resulting in suboptimal policies. Recovering from such setbacks may prove arduous, if not impossible, potentially prolonging the training process or hindering progress altogether.\n",
    "\n",
    "  \n",
    "In PPO, policy updates are approached with a conservative mindset. This entails quantifying the disparity between the current policy and its predecessor through a calculated ratio. By constraining this ratio within a defined range [1-ε, 1+ε] (**one minus epsilon, comma, one plus epsilon**), we effectively eliminate the temptation for the current policy to stray too far from its predecessor. This encapsulates the essence of the \"proximal policy\" notion, as we aim to keep policy adjustments within a proximate vicinity of the previous policy, thus fostering stability and mitigating drastic deviations.\n",
    "\n",
    "## The Clipped Surrogate Objective Function\n",
    "This was the objective to optmize in our reinforce algorithim:\n",
    "![enter image description here](https://live.staticflickr.com/65535/53540221027_1e500d0a76_z.jpg)\n",
    "  \n",
    "The idea was to help our agent choose actions that get better rewards and avoid bad ones. We tried to do this by adjusting our strategy using a math formula. But we ran into a problem with how big of a step we should take:\n",
    "\n",
    "-   If we took small steps, it took forever to train.\n",
    "-   If we took big steps, things got too unpredictable due to high variance.\n",
    "\n",
    "With PPO, we came up with a new plan. Instead of changing our strategy too much at once, we use a method called the Clipped Surrogate Objective Function. This method keeps our strategy changes in check, preventing them from getting too extreme. It helps us avoid making big mistakes during training.\n",
    "\n",
    "This new function **is designed to avoid destructively large weights updates** :\n",
    "![enter image description here](https://live.staticflickr.com/65535/53541114956_ae2e267c31_b.jpg)\n",
    "\n",
    "## Let's dive deeper and understnad this equation:\n",
    "### The Ratio Function\n",
    "![enter image description here](https://live.staticflickr.com/65535/53541314648_ce9f327921_b.jpg)\n",
    "\n",
    "The ratio is represented by this equation:\n",
    "![enter image description here](https://live.staticflickr.com/65535/53541564455_0a91b38977_b.jpg)\n",
    "\n",
    "This is about the likelihood of choosing an action 'a' at state 's' in the current policy compared to the previous one.\n",
    "\n",
    "The probability ratio, denoted by 'r(θ)', tells us:\n",
    "\n",
    "-   If **r(θ) > 1**, it means the action 'a' at state 's' is more probable in the current policy than the old one.\n",
    "-   If r(θ) is between 0 and 1, it means the action is less likely in the current policy than before.\n",
    "\n",
    "So, this probability ratio helps us understand how much the current policy differs from the old one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae00f68-bb1d-4672-b758-18ac37f8e5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
